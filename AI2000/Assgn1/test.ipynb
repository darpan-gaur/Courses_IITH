{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thetaP: [0.33333333 0.16666667 0.16666667 0.83333333 0.83333333 0.16666667\n",
      " 0.66666667 0.83333333]\n",
      "thetaS: [0.66666667 0.66666667 0.16666667 0.66666667 0.16666667 0.16666667\n",
      " 0.         0.16666667]\n",
      "[0.33333333 1.         1.         0.83333333 0.83333333 0.16666667\n",
      " 0.66666667 1.        ]\n",
      "[1.         0.83333333 0.83333333 1.         1.         1.\n",
      " 1.         0.16666667]\n",
      "yP: 0.001488435451912818\n",
      "yS: 0.0\n",
      "Posterior probability that the document is about politics: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training data for politics and sport documents\n",
    "xP = np.array([\n",
    "    [1, 0, 1, 1, 1, 0, 1, 1],  # Politics\n",
    "    [0, 0, 0, 1, 0, 0, 1, 1],\n",
    "    [1, 0, 0, 1, 1, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 1, 1, 0, 1],\n",
    "    [0, 0, 0, 1, 1, 0, 1, 1],\n",
    "    [0, 0, 0, 1, 1, 0, 0, 1]\n",
    "])\n",
    "\n",
    "xS = np.array([\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0],  # Sport\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 1, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 1, 0, 0, 0, 1],\n",
    "    [1, 1, 0, 1, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 1, 0, 0]\n",
    "])\n",
    "\n",
    "# Test point\n",
    "x = np.array([1, 0, 0, 1, 1, 1, 1, 0])\n",
    "\n",
    "# ML estimates of p(x=1|class)\n",
    "thetaP = np.mean(xP, axis=0)\n",
    "thetaS = np.mean(xS, axis=0)\n",
    "\n",
    "print(f\"thetaP: {thetaP}\")\n",
    "print(f\"thetaS: {thetaS}\")\n",
    "\n",
    "# ML class priors\n",
    "pP = xP.shape[0] / (xP.shape[0] + xS.shape[0])\n",
    "pS = 1 - pP\n",
    "\n",
    "# Bayesian numerators: prob(x|xP)*pP and prob(x|xS)*pS\n",
    "yP = np.prod(thetaP**x * (1 - thetaP)**(1 - x)) * pP\n",
    "yS = np.prod(thetaS**x * (1 - thetaS)**(1 - x)) * pS\n",
    "\n",
    "print(thetaP**x)\n",
    "print((1 - thetaP)**(1 - x))\n",
    "\n",
    "print(f\"yP: {yP}\")\n",
    "print(f\"yS: {yS}\")\n",
    "\n",
    "# Posterior probability of the document being about politics\n",
    "prob_politics = yP / (yP + yS)\n",
    "\n",
    "print(\"Posterior probability that the document is about politics:\", prob_politics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./wine+quality/winequality-red.csv', sep=';')\n",
    "df.rename(columns={\"quality\": \"label\"}, inplace=True)\n",
    "# label <7 = 0, label >=7 = 1\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x >= 7 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1279, 12)\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test sets\n",
    "def split_data(data, split_ratio=0.8, seed=None):\n",
    "    # get 80% train and 20% test split, randomly, specifying seed for reproducibility\n",
    "    if (seed):\n",
    "        train = data.sample(frac=split_ratio, random_state=seed)\n",
    "    else:\n",
    "        train = data.sample(frac=split_ratio)\n",
    "    test = data.drop(train.index)\n",
    "    return train, test\n",
    "\n",
    "train, test = split_data(df)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>11.5</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.084</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.99870</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.70</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.080</td>\n",
       "      <td>31.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.99622</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>8.7</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.226</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.99910</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.60</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.064</td>\n",
       "      <td>12.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.99290</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.59</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>9.1</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.058</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.99392</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.55</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "464            11.5             0.315         0.54             2.1      0.084   \n",
       "1561            7.8             0.600         0.26             2.0      0.080   \n",
       "567             8.7             0.700         0.24             2.5      0.226   \n",
       "1311            6.5             0.510         0.15             3.0      0.064   \n",
       "1322            9.1             0.340         0.42             1.8      0.058   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "464                   5.0                  15.0  0.99870  2.98       0.70   \n",
       "1561                 31.0                 131.0  0.99622  3.21       0.52   \n",
       "567                   5.0                  15.0  0.99910  3.32       0.60   \n",
       "1311                 12.0                  27.0  0.99290  3.33       0.59   \n",
       "1322                  9.0                  18.0  0.99392  3.18       0.55   \n",
       "\n",
       "      alcohol  label  \n",
       "464       9.2      0  \n",
       "1561      9.9      0  \n",
       "567       9.0      0  \n",
       "1311     12.8      0  \n",
       "1322     11.4      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "train, test = split_data(df)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\n",
      "[ 8.4         8.5         8.7         8.8         9.          9.05\n",
      "  9.1         9.2         9.23333333  9.25        9.3         9.4\n",
      "  9.5         9.55        9.56666667  9.6         9.7         9.8\n",
      "  9.9         9.95       10.         10.03333333 10.1        10.2\n",
      " 10.3        10.4        10.5        10.55       10.6        10.7\n",
      " 10.75       10.8        10.9        11.         11.1        11.2\n",
      " 11.3        11.4        11.5        11.6        11.7        11.8\n",
      " 11.9        11.95       12.         12.1        12.2        12.3\n",
      " 12.4        12.5        12.6        12.7        12.8        12.9\n",
      " 13.         13.1        13.2        13.3        13.4        13.5\n",
      " 13.6        14.         14.9       ]\n"
     ]
    }
   ],
   "source": [
    "# print count unique values in each feature\n",
    "print(len(train['alcohol']))\n",
    "print(np.unique(train['alcohol']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, gain=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate decision tree using entropy and information gain\n",
    "class decisionTrees:\n",
    "    def __init__(self, data, target, criterion='entropy'):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def entropy(self, data):\n",
    "        # get unique values and counts\n",
    "        unique, counts = np.unique(data, return_counts=True)\n",
    "        # calculate probability of each unique value\n",
    "        prob = counts / len(data)\n",
    "        # calculate entropy\n",
    "        entropy = -np.sum(prob * np.log2(prob))\n",
    "        return entropy\n",
    "    \n",
    "    def entropy2(self, data_l, data_r):\n",
    "        # calculate entropy\n",
    "        n = len(data_l) + len(data_r)\n",
    "        p_l = len(data_l) / n\n",
    "        p_r = len(data_r) / n\n",
    "        entropy = p_l * self.entropy(data_l) + p_r * self.entropy(data_r)\n",
    "        return entropy\n",
    "    \n",
    "    def information_gain(self, data, feature):\n",
    "        # get unique values and counts\n",
    "        unique, counts = np.unique(data[feature], return_counts=True)\n",
    "        # calculate probability of each unique value\n",
    "        prob = counts / len(data)\n",
    "        # calculate entropy for each unique value\n",
    "        entropy = np.array([self.entropy(data[data[feature] == u][self.target]) for u in unique])\n",
    "        # calculate information gain\n",
    "        information_gain = np.sum(prob * entropy)\n",
    "        return information_gain\n",
    "    \n",
    "    def classify(self, data):\n",
    "        # get unique values and counts\n",
    "        unique, counts = np.unique(data[self.target], return_counts=True)\n",
    "        # return majority class\n",
    "        return unique[np.argmax(counts)]\n",
    "    \n",
    "    def potential_splits(self, data):\n",
    "        potential_splits = {}\n",
    "        for column in data.columns:\n",
    "            if column != self.target:\n",
    "                potential_splits[column] = np.unique(data[column])\n",
    "        return potential_splits\n",
    "    \n",
    "    def split_data(self, data, feature, value):\n",
    "        # return left and right splits\n",
    "        return data[data[feature] <= value], data[data[feature] > value]\n",
    "    \n",
    "    def get_best_split(self, data):\n",
    "        potential_splits = self.potential_splits(data)\n",
    "        best_split = {}\n",
    "        best_info_gain = np.inf\n",
    "        for feature in potential_splits:\n",
    "            for value in potential_splits[feature]:\n",
    "                data_l, data_r = self.split_data(data, feature, value)\n",
    "                current_info_gain = self.entropy2(data_l[self.target], data_r[self.target])\n",
    "                if current_info_gain < best_info_gain:\n",
    "                    best_info_gain = current_info_gain\n",
    "                    best_split = {'feature': feature, 'value': value}\n",
    "        return best_split\n",
    "    \n",
    "    def tree_maker(self, data, curr_depth=0, min_samples=2, max_depth=5):\n",
    "        if len(data) < min_samples or curr_depth == max_depth:\n",
    "            return self.classify(data)\n",
    "        \n",
    "        curr_depth += 1\n",
    "\n",
    "        split = self.get_best_split(data)\n",
    "        data_l, data_r = self.split_data(data, split['feature'], split['value'])\n",
    "        question = f\"{split['feature']} <= {split['value']}\"\n",
    "        sub_tree = {question: []}\n",
    "\n",
    "        yes = self.tree_maker(data_l, curr_depth)\n",
    "        no = self.tree_maker(data_r, curr_depth)\n",
    "\n",
    "        if yes == no:\n",
    "            sub_tree = yes\n",
    "        else:\n",
    "            sub_tree[question].append(yes)\n",
    "            sub_tree[question].append(no)\n",
    "\n",
    "        return sub_tree\n",
    "\n",
    "tree = decisionTrees(train, 'label')\n",
    "tree1 = tree.tree_maker(train, max_depth=3)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     accuracy \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mclassification_correct\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m accuracy\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m accuracy \u001b[39m=\u001b[39m calculate_accuracy(test, tree1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_accuracy\u001b[39m(data, tree):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mapply(classify_example, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, args\u001b[39m=\u001b[39;49m(tree,))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mclassification_correct\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     accuracy \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mclassification_correct\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/Documents/.venv/test/lib/python3.11/site-packages/pandas/core/frame.py:10037\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10025\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10027\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m  10028\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m  10029\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10035\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m  10036\u001b[0m )\n\u001b[0;32m> 10037\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/.venv/test/lib/python3.11/site-packages/pandas/core/apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    835\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 837\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/Documents/.venv/test/lib/python3.11/site-packages/pandas/core/apply.py:963\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 963\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    965\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/Documents/.venv/test/lib/python3.11/site-packages/pandas/core/apply.py:979\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    977\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    978\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 979\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(v, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m    980\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    981\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    982\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    983\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m answer\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m classify_example(example, answer)\n",
      "\u001b[1;32m/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m answer\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m classify_example(example, answer)\n",
      "\u001b[1;32m/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify_example\u001b[39m(example, tree):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     question \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tree\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     feature, comparison, value \u001b[39m=\u001b[39m question\u001b[39m.\u001b[39msplit()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m example[feature] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(value):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darpan/Desktop/7thSem/AI2000/Assgn1/co21btech11004_Assgn1.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         answer \u001b[39m=\u001b[39m tree[question][\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "def classify_example(example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    feature, comparison, value = question.split()\n",
    "    if example[feature] <= float(value):\n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    else:\n",
    "        return classify_example(example, answer)\n",
    "    \n",
    "def calculate_accuracy(data, tree):\n",
    "    data['classification'] = data.apply(classify_example, axis=1, args=(tree,))\n",
    "    data['classification_correct'] = data['classification'] == data['label']\n",
    "    accuracy = data['classification_correct'].mean()\n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_accuracy(test, tree1)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"\n",
    "    A class representing a node in a decision tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, gain=None, value=None):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the Node class.\n",
    "\n",
    "        Args:\n",
    "            feature: The feature used for splitting at this node. Defaults to None.\n",
    "            threshold: The threshold used for splitting at this node. Defaults to None.\n",
    "            left: The left child node. Defaults to None.\n",
    "            right: The right child node. Defaults to None.\n",
    "            gain: The gain of the split. Defaults to None.\n",
    "            value: If this node is a leaf node, this attribute represents the predicted value\n",
    "                for the target variable. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.gain = gain\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    A decision tree classifier for binary classification problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_samples=2, max_depth=2):\n",
    "        \"\"\"\n",
    "        Constructor for DecisionTree class.\n",
    "\n",
    "        Parameters:\n",
    "            min_samples (int): Minimum number of samples required to split an internal node.\n",
    "            max_depth (int): Maximum depth of the decision tree.\n",
    "        \"\"\"\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def split_data(self, dataset, feature, threshold):\n",
    "        \"\"\"\n",
    "        Splits the given dataset into two datasets based on the given feature and threshold.\n",
    "\n",
    "        Parameters:\n",
    "            dataset (ndarray): Input dataset.\n",
    "            feature (int): Index of the feature to be split on.\n",
    "            threshold (float): Threshold value to split the feature on.\n",
    "\n",
    "        Returns:\n",
    "            left_dataset (ndarray): Subset of the dataset with values less than or equal to the threshold.\n",
    "            right_dataset (ndarray): Subset of the dataset with values greater than the threshold.\n",
    "        \"\"\"\n",
    "        # Create empty arrays to store the left and right datasets\n",
    "        left_dataset = []\n",
    "        right_dataset = []\n",
    "        \n",
    "        # Loop over each row in the dataset and split based on the given feature and threshold\n",
    "        for row in dataset:\n",
    "            if row[feature] <= threshold:\n",
    "                left_dataset.append(row)\n",
    "            else:\n",
    "                right_dataset.append(row)\n",
    "\n",
    "        # Convert the left and right datasets to numpy arrays and return\n",
    "        left_dataset = np.array(left_dataset)\n",
    "        right_dataset = np.array(right_dataset)\n",
    "        return left_dataset, right_dataset\n",
    "\n",
    "    def entropy(self, y):\n",
    "        \"\"\"\n",
    "        Computes the entropy of the given label values.\n",
    "\n",
    "        Parameters:\n",
    "            y (ndarray): Input label values.\n",
    "\n",
    "        Returns:\n",
    "            entropy (float): Entropy of the given label values.\n",
    "        \"\"\"\n",
    "        entropy = 0\n",
    "\n",
    "        # Find the unique label values in y and loop over each value\n",
    "        labels = np.unique(y)\n",
    "        for label in labels:\n",
    "            # Find the examples in y that have the current label\n",
    "            label_examples = y[y == label]\n",
    "            # Calculate the ratio of the current label in y\n",
    "            pl = len(label_examples) / len(y)\n",
    "            # Calculate the entropy using the current label and ratio\n",
    "            entropy += -pl * np.log2(pl)\n",
    "\n",
    "        # Return the final entropy value\n",
    "        return entropy\n",
    "\n",
    "    def information_gain(self, parent, left, right):\n",
    "        \"\"\"\n",
    "        Computes the information gain from splitting the parent dataset into two datasets.\n",
    "\n",
    "        Parameters:\n",
    "            parent (ndarray): Input parent dataset.\n",
    "            left (ndarray): Subset of the parent dataset after split on a feature.\n",
    "            right (ndarray): Subset of the parent dataset after split on a feature.\n",
    "\n",
    "        Returns:\n",
    "            information_gain (float): Information gain of the split.\n",
    "        \"\"\"\n",
    "        # set initial information gain to 0\n",
    "        information_gain = 0\n",
    "        # compute entropy for parent\n",
    "        parent_entropy = self.entropy(parent)\n",
    "        # calculate weight for left and right nodes\n",
    "        weight_left = len(left) / len(parent)\n",
    "        weight_right= len(right) / len(parent)\n",
    "        # compute entropy for left and right nodes\n",
    "        entropy_left, entropy_right = self.entropy(left), self.entropy(right)\n",
    "        # calculate weighted entropy \n",
    "        weighted_entropy = weight_left * entropy_left + weight_right * entropy_right\n",
    "        # calculate information gain \n",
    "        information_gain = parent_entropy - weighted_entropy\n",
    "        return information_gain\n",
    "\n",
    "    \n",
    "    def best_split(self, dataset, num_samples, num_features):\n",
    "        \"\"\"\n",
    "        Finds the best split for the given dataset.\n",
    "\n",
    "        Args:\n",
    "        dataset (ndarray): The dataset to split.\n",
    "        num_samples (int): The number of samples in the dataset.\n",
    "        num_features (int): The number of features in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary with the best split feature index, threshold, gain, \n",
    "              left and right datasets.\n",
    "        \"\"\"\n",
    "        # dictionary to store the best split values\n",
    "        best_split = {'gain':- 1, 'feature': None, 'threshold': None}\n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            #get the feature at the current feature_index\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            #get unique values of that feature\n",
    "            thresholds = np.unique(feature_values)\n",
    "            # loop over all values of the feature\n",
    "            for threshold in thresholds:\n",
    "                # get left and right datasets\n",
    "                left_dataset, right_dataset = self.split_data(dataset, feature_index, threshold)\n",
    "                # check if either datasets is empty\n",
    "                if len(left_dataset) and len(right_dataset):\n",
    "                    # get y values of the parent and left, right nodes\n",
    "                    y, left_y, right_y = dataset[:, -1], left_dataset[:, -1], right_dataset[:, -1]\n",
    "                    # compute information gain based on the y values\n",
    "                    information_gain = self.information_gain(y, left_y, right_y)\n",
    "                    # update the best split if conditions are met\n",
    "                    if information_gain > best_split[\"gain\"]:\n",
    "                        best_split[\"feature\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"left_dataset\"] = left_dataset\n",
    "                        best_split[\"right_dataset\"] = right_dataset\n",
    "                        best_split[\"gain\"] = information_gain\n",
    "        return best_split\n",
    "\n",
    "    \n",
    "    def calculate_leaf_value(self, y):\n",
    "        \"\"\"\n",
    "        Calculates the most occurring value in the given list of y values.\n",
    "\n",
    "        Args:\n",
    "            y (list): The list of y values.\n",
    "\n",
    "        Returns:\n",
    "            The most occurring value in the list.\n",
    "        \"\"\"\n",
    "        y = list(y)\n",
    "        #get the highest present class in the array\n",
    "        most_occuring_value = max(y, key=y.count)\n",
    "        return most_occuring_value\n",
    "    \n",
    "    def build_tree(self, dataset, current_depth=0):\n",
    "        \"\"\"\n",
    "        Recursively builds a decision tree from the given dataset.\n",
    "\n",
    "        Args:\n",
    "        dataset (ndarray): The dataset to build the tree from.\n",
    "        current_depth (int): The current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "        Node: The root node of the built decision tree.\n",
    "        \"\"\"\n",
    "        # split the dataset into X, y values\n",
    "        X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        n_samples, n_features = X.shape\n",
    "        # keeps spliting until stopping conditions are met\n",
    "        if n_samples >= self.min_samples and current_depth <= self.max_depth:\n",
    "            # Get the best split\n",
    "            best_split = self.best_split(dataset, n_samples, n_features)\n",
    "            # Check if gain isn't zero\n",
    "            if best_split[\"gain\"]:\n",
    "                # continue splitting the left and the right child. Increment current depth\n",
    "                left_node = self.build_tree(best_split[\"left_dataset\"], current_depth + 1)\n",
    "                right_node = self.build_tree(best_split[\"right_dataset\"], current_depth + 1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature\"], best_split[\"threshold\"],\n",
    "                            left_node, right_node, best_split[\"gain\"])\n",
    "\n",
    "        # compute leaf node value\n",
    "        leaf_value = self.calculate_leaf_value(y)\n",
    "        # return leaf node value\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Builds and fits the decision tree to the given X and y values.\n",
    "\n",
    "        Args:\n",
    "        X (ndarray): The feature matrix.\n",
    "        y (ndarray): The target values.\n",
    "        \"\"\"\n",
    "        dataset = np.concatenate((X, y), axis=1)  \n",
    "        self.root = self.build_tree(dataset)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for each instance in the feature matrix X.\n",
    "\n",
    "        Args:\n",
    "        X (ndarray): The feature matrix to make predictions for.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of predicted class labels.\n",
    "        \"\"\"\n",
    "        # Create an empty list to store the predictions\n",
    "        predictions = []\n",
    "        # For each instance in X, make a prediction by traversing the tree\n",
    "        for x in X:\n",
    "            prediction = self.make_prediction(x, self.root)\n",
    "            # Append the prediction to the list of predictions\n",
    "            predictions.append(prediction)\n",
    "        # Convert the list to a numpy array and return it\n",
    "        np.array(predictions)\n",
    "        return predictions\n",
    "    \n",
    "    def make_prediction(self, x, node):\n",
    "        \"\"\"\n",
    "        Traverses the decision tree to predict the target value for the given feature vector.\n",
    "\n",
    "        Args:\n",
    "        x (ndarray): The feature vector to predict the target value for.\n",
    "        node (Node): The current node being evaluated.\n",
    "\n",
    "        Returns:\n",
    "        The predicted target value for the given feature vector.\n",
    "        \"\"\"\n",
    "        # if the node has value i.e it's a leaf node extract it's value\n",
    "        if node.value != None: \n",
    "            return node.value\n",
    "        else:\n",
    "            #if it's node a leaf node we'll get it's feature and traverse through the tree accordingly\n",
    "            feature = x[node.feature]\n",
    "            if feature <= node.threshold:\n",
    "                return self.make_prediction(x, node.left)\n",
    "            else:\n",
    "                return self.make_prediction(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of a classification model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "        y_true (numpy array): A numpy array of true labels for each data point.\n",
    "        y_pred (numpy array): A numpy array of predicted labels for each data point.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "        float: The accuracy of the model\n",
    "    \"\"\"\n",
    "    y_true = y_true.flatten()\n",
    "    total_samples = len(y_true)\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    return (correct_predictions / total_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTree(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train.drop('label', axis=1).values, train['label'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test.drop('label', axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Accuracy: 0.859375\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's Accuracy: {accuracy(test['label'].values, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a plot for genralization error\n",
    "X = np.array([0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50])\n",
    "# plt.plot(X)\n",
    "plt.plot()\n",
    "plt.xlim(0, 50)\n",
    "plt.ylim(0, 10)\n",
    "plt.xlabel(' k ')\n",
    "plt.ylabel('Generalization Error')\n",
    "plt.title('Generalization Error vs k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
